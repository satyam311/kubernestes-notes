### Static Pods, Manual Scheduling, Labels, and Selectors in Kubernetes

my notes : 

In Kubernetes, the scheduler (part of the control plane) is responsible for scheduling pods on worker nodes — it decides which node a pod should run on. The kubelet on each node then takes that scheduling decision and runs the pod.

However, all core control plane components like the API Server, Controller Manager, Scheduler, and etcd themselves also run as pods.
This raises a question:

Who schedules these control plane pods if the scheduler itself is not yet running?

That’s where Static Pods come into the picture.

The kubelet running on the control plane node automatically manages these Static Pods. It continuously monitors a specific directory (usually /etc/kubernetes/manifests) for pod manifest files.
Whenever a YAML manifest is found there, the kubelet directly creates and runs that pod on the same node — without using the API server or the scheduler.

This ensures that even before the control plane is fully functional, the essential components can start up automatically.

```kubectl get ns```

```
NAME                 STATUS   AGE
default              Active   4d12h
kube-node-lease      Active   4d12h
kube-public          Active   4d12h
kube-system          Active   4d12h
local-path-storage   Active   4d12h
```
```
kubectl get pods -n kube-system | grep scheduler
kube-scheduler-multi-node-cluster-control-plane            1/1     Running   1 (35h ago)   4d12h
```
```
kubectl get pods kube-scheduler-multi-node-cluster-control-plane -n kube-system -o wide 

NAME                                              READY   STATUS    RESTARTS      AGE     IP           NODE                               NOMINATED NODE   READINESS GATES
kube-scheduler-multi-node-cluster-control-plane   1/1     Running   1 (35h ago)   4d12h   172.20.0.5   multi-node-cluster-control-plane   <none>           <none>
```

```
docker exec -it multi-node-cluster-control-plane bash
/# cd  /etc/kubernetes/manifests
ls -ltr
-rw------- 1 root root 2632 Oct 14 07:47 etcd.yaml
-rw------- 1 root root 3968 Oct 14 07:47 kube-apiserver.yaml
-rw------- 1 root root 3507 Oct 14 07:47 kube-controller-manager.yaml
-rw------- 1 root root 1726 Oct 14 07:47 kube-scheduler.yaml
mv kube-scheduler.yaml /tmp

```

```

kubectl run nginx-pod --image=nginx
kubectl get pods
kubectl get pods | nginx-pod

nginx-pod              0/1     Pending     0

```
```

mv /tmp/kube-scheduler.yaml .

kubectl get pods | nginx-pod
nginx-pod              1/1     Running     0             4m26s

```
--- 

### Manual Scehduling 

Even if we remove the scheduler node from the control plane, we can manually scheule our pod on the desired node using ```nodeName``` in the yaml manifest

```
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: nginx-pod
  name: nginx-pod
  namespace: default
spec:
  containers:
  - image: nginx
    name: nginx-pod
  nodeName: multi-node-cluster-worker
```

--- 

### How labels, MatchLabels used in yaml of deploymemnt, service , rs etc ? 

```
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: ds-set-deploy-name
  name: ds-name
spec:
  replicas: 2
  selector:
    matchLabels:
      app: ds-set
  template:
    metadata:
      labels:
        app: ds-set
    spec:
      containers:
      - image: nginx
        name: nginx
```


**Labels** are key-value pairs attached to Kubernetes objects (like Pods, Services, Deployments, etc.) to organize and identify them.

They don’t affect how the object behaves — they’re just used for grouping or selection.

A **selector** is how Kubernetes finds or filters objects based on their labels.


```kubectl get pods --selector app=ds-set```
```kubectl get pods -l app=ds-set```
```kubectl get pods -l 'env in (prod,stage)' ```

**Matchlabels** :


matchLabels is a field used inside resources like Deployments, ReplicaSets, or StatefulSets — it defines which labels on Pods must match for them to be managed by that controller.


```spec.selector.matchLabels``` should match the ```spec.template.metadata.labels``` otherwise the Deployment will not manage those pods.










